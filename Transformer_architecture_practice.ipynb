{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a8372d-c1a0-44d8-b93a-11980665da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78cb9119-951b-4bfa-aaf6-b56cb8d8d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "EMBEDDING_DIM = 720\n",
    "BLOCK_SIZE = 16\n",
    "N_HEADS = 16\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b297336b-ff1d-4d9b-9534-14f917f73b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM // N_HEADS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceacb266-39e1-44f8-b64c-5f5e953c471c",
   "metadata": {},
   "source": [
    "# Loading text and removing unnecessary characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae272457-3085-4c27-9e45-0443ecd2966c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "remove = ['-', '$', '&']\n",
    "for rm in remove:\n",
    "    text = text.replace(rm, '')\n",
    "words = text.split()\n",
    "\n",
    "VOCAB_SIZE = len(set(words))\n",
    "vocab = sorted(list(set(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ddaa2e3-f9c3-4e39-a776-44be88c0e70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25454"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe86d244-5a30-4970-9abc-74711e4f5e49",
   "metadata": {},
   "source": [
    "# Training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9747c24-86c6-4b48-a1e8-d2796761b0bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_int = {word:idx for idx, word in enumerate(vocab)}\n",
    "int_vocab = {idx:word for idx, word in enumerate(vocab)}\n",
    "\n",
    "word_to_int_array = [vocab_int[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81938cf7-271f-4373-b6a4-2ffb5d6e4e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_size = len(words)\n",
    "inputs = list()\n",
    "targets = list()\n",
    "for i in range(10000):\n",
    "    start = numpy.random.randint(1,\n",
    "                                 list_size - BLOCK_SIZE - 1\n",
    "                                )\n",
    "    inputs.append(word_to_int_array[start:start+BLOCK_SIZE])\n",
    "    targets.append(word_to_int_array[start+BLOCK_SIZE+1])\n",
    "\n",
    "\n",
    "inputs = torch.tensor(inputs, \n",
    "                      dtype = torch.long, \n",
    "                      device = device\n",
    "                     )\n",
    "targets = torch.tensor(targets, \n",
    "                       dtype = torch.long, \n",
    "                       device = device\n",
    "                      )    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03758829-d5d7-450a-9821-44c76c24d0ab",
   "metadata": {},
   "source": [
    "# Transformer Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa79cd33-319c-4e2c-ac58-3cbe02ab7059",
   "metadata": {},
   "source": [
    "## Embedding block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "872ab86b-eb30-4aeb-bea8-20b82c24fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding Block:\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    vocab_size(int): Size of vocabulary of document\n",
    "    embd(int): Size of embedding dimension\n",
    "    block_size(int): Number of elements in each row of input\n",
    "\n",
    "    Description:\n",
    "    ------------\n",
    "\n",
    "    To represent each number(word) with a unique sequence of numbers\n",
    "    which the computer can understand. Along with this, positional \n",
    "    information is also represented in the same higher dimension and \n",
    "    added to the embedding tensor.\n",
    "\n",
    "    Input dim: B,T\n",
    "    Output dim: B,T,H (H = embedding dimension)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embd, block_size, dim_journey):\n",
    "        super().__init__()\n",
    "        self.dim_journey = dim_journey\n",
    "        # Embedding layer\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings = vocab_size, \n",
    "                                            embedding_dim =  embd\n",
    "                                           )\n",
    "        # Positional embedding layer\n",
    "        self.pos_layer = nn.Embedding(num_embeddings = block_size, \n",
    "                                      embedding_dim = embd\n",
    "                                     )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        if self.dim_journey:\n",
    "            print(\"\\x1b[22;31mEmbedding Block\\x1b[0m\")\n",
    "            print(f\"\\x1b[32mInput dimension:\\x1b[0m {x.shape}\")\n",
    "        embeddings = self.embedding_layer(x)\n",
    "        pos = self.pos_layer(torch.arange(T, \n",
    "                                          device = device))\n",
    "        token_embd = embeddings + pos\n",
    "\n",
    "        if self.dim_journey:\n",
    "            print(f\"\\x1b[32mDimension of Embedding layer:\\x1b[0m {embeddings.shape}\")\n",
    "            print(f\"\\x1b[32mDimensions of Positional layer:\\x1b[0m {pos.shape}\")\n",
    "            print(f\"\\x1b[32mDimensions of Token embeddings:\\x1b[0m {token_embd.shape}\")\n",
    "\n",
    "        return token_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "784afd92-c52a-4691-b9af-482e2241992e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[22;31mEmbedding Block\u001b[0m\n",
      "\u001b[32mInput dimension:\u001b[0m torch.Size([3, 16])\n",
      "\u001b[32mDimension of Embedding layer:\u001b[0m torch.Size([3, 16, 720])\n",
      "\u001b[32mDimensions of Positional layer:\u001b[0m torch.Size([16, 720])\n",
      "\u001b[32mDimensions of Token embeddings:\u001b[0m torch.Size([3, 16, 720])\n"
     ]
    }
   ],
   "source": [
    "m = EmbeddingBlock(vocab_size = VOCAB_SIZE, embd = EMBEDDING_DIM, block_size = BLOCK_SIZE, dim_journey=True)\n",
    "m.to(device)\n",
    "temp = m(inputs[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b70001-d987-4c7a-8f77-1a42179567ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51b4b45d-4893-4810-bc06-f560d0613864",
   "metadata": {},
   "source": [
    "## Encoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956eea67-88cd-43eb-aa78-bf2bcfd7ce50",
   "metadata": {},
   "source": [
    "### Multi-Head Attention Block\n",
    "\n",
    "Head Block -> Multi-Head Block(Feed Forward Block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99adcf6e-023d-48f9-8639-e35f6cfca9cf",
   "metadata": {},
   "source": [
    "#### Feed Forward Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1e72421-feeb-492e-834f-01e4f821a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    FeedForward Block:\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    embd(int): Size of embedding dimension\n",
    "    \n",
    "    Description:\n",
    "    ------------\n",
    "    A simple linear layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(in_features = embd, \n",
    "                                          out_features = embd),\n",
    "                                 nn.ReLU()\n",
    "                                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb966fe5-c849-43a0-a6f7-5ec405f0efc2",
   "metadata": {},
   "source": [
    "#### Head Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0e8b735-6d03-4029-85f7-ce6f3f59d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Head Block:\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    head_size(int): Number of heads in self attention\n",
    "    embd(int): Embedding dimension\n",
    "    block_size(int): Number of elements in single row\n",
    "    dim_journey(bool): Explanation of dimension conversions through\n",
    "    each block and within each block as well.\n",
    "    \n",
    "    Description:\n",
    "    ------------\n",
    "\n",
    "    Core block of Transformer. Here is where the attention \n",
    "    mechanism is implemented. This is a single \"head\" of the \n",
    "    transformer. In the Multi-Head block a number of these heads are\n",
    "    created and each learns different information about the text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, embd, block_size, dim_journey):\n",
    "        super().__init__()\n",
    "        self.dim_journey = dim_journey\n",
    "        self.key = nn.Linear(in_features = embd, \n",
    "                             out_features = head_size, \n",
    "                             bias = False)\n",
    "        self.query = nn.Linear(in_features = embd, \n",
    "                               out_features = head_size, \n",
    "                               bias = False)\n",
    "        self.value = nn.Linear(in_features = embd, \n",
    "                               out_features = head_size, \n",
    "                               bias = False)\n",
    "        self.register_buffer('tril', \n",
    "                             torch.tril(torch.ones(block_size, \n",
    "                                                   block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape            \n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        weights = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        # Masking makes it a decoder block\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, \n",
    "                                      float('-inf'))\n",
    "        weights = F.softmax(weights, dim = -1)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        out = weights @ v\n",
    "        if self.dim_journey:\n",
    "            print(\"\\x1b[22;31mHead Block\\x1b[0m\")\n",
    "            print(f\"\\x1b[32mDimensions of input:\\x1b[0m {x.shape}\")\n",
    "            print(f\"\\x1b[32mDimensions of key:\\x1b[0m {k.shape}\")\n",
    "            print(f\"\\x1b[32mDimensions of query:\\x1b[0m {q.shape}\")\n",
    "            print(f\"\\x1b[32mDimensions of value:\\x1b[0m {v.shape}\")\n",
    "            print(f\"\\x1b[32mDimensions of weights after q @ k.T:\\x1b[0m {weights.shape}\")\n",
    "            print(f\"\\x1b[32mDimensions of out(weights @ v):\\x1b[0m {out.shape}\")\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d725c-ef5c-4246-99a6-74d094762a80",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55ddbd3b-8c41-410e-8e30-4dd42e2221e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM / N_HEADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c5beb4e-150b-49ec-bea5-90152d78de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Block:\n",
    "\n",
    "    Multiple attention heads running in parallel.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "\n",
    "    Description:\n",
    "    ------------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embd, n_head, block_size, dim_journey):\n",
    "        super().__init__()\n",
    "        head_kwargs = {\"head_size\": embd // n_head,\n",
    "                      \"embd\": embd,\n",
    "                      \"block_size\": block_size,\n",
    "                      \"dim_journey\": dim_journey}\n",
    "        self.dim_journey = dim_journey\n",
    "        self.heads = nn.ModuleList([HeadBlock(**head_kwargs) for _ in range(n_head)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        concat = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        if self.dim_journey:\n",
    "            print(\"\\x1b[22;31mMultiHead Block\\x1b[0m\")\n",
    "            print(f\"Dimensions of output: {concat.shape}\")\n",
    "        return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab9e066-cf53-4e08-ad55-4e87fdff91bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d69104d7-dcef-4b15-a863-7219f991a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Block:\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "\n",
    "    Description:\n",
    "    ------------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2316d084-b736-4170-bcc8-2c5008d12cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "481263c1-6221-4d6e-9305-73cbcdf1d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    vocab_size(int): Size of vocabulary of document.\n",
    "    embd(int): Size of embedding dimension.\n",
    "    block_size(int): Number of elements in each row of input.\n",
    "    n_head(int): Number of heads for Multi-head Attention block.\n",
    "    dim_journey(bool): Explanation of dimension conversions through\n",
    "    each block and within each block as well.\n",
    "\n",
    "    Description:\n",
    "    ------------\n",
    "\n",
    "    Embedding layer:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embd, block_size, n_head, dim_journey = False):\n",
    "        super().__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding_layer = EmbeddingBlock(vocab_size = vocab_size, \n",
    "                                              embd = embd, \n",
    "                                              block_size = block_size, \n",
    "                                              dim_journey = dim_journey)\n",
    "        self.sa_heads = MultiHeadBlock(n_head = n_head, \n",
    "                                       embd = embd, \n",
    "                                       block_size = block_size, \n",
    "                                       dim_journey = dim_journey)\n",
    "        # Linear head layer\n",
    "        self.lm_head = nn.Linear(in_features = embd, \n",
    "                                 out_features = vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.sa_heads(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d8d036b-ef1c-4c14-a3cd-cb652aab543c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[22;31mEmbedding Block\u001b[0m\n",
      "\u001b[32mInput dimension:\u001b[0m torch.Size([1, 16])\n",
      "\u001b[32mDimension of Embedding layer:\u001b[0m torch.Size([1, 16, 720])\n",
      "\u001b[32mDimensions of Positional layer:\u001b[0m torch.Size([16, 720])\n",
      "\u001b[32mDimensions of Token embeddings:\u001b[0m torch.Size([1, 16, 720])\n",
      "\u001b[22;31mHead Block\u001b[0m\n",
      "\u001b[32mDimensions of input:\u001b[0m torch.Size([1, 16, 720])\n",
      "\u001b[32mDimensions of key:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of query:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of value:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of weights after q @ k.T:\u001b[0m torch.Size([1, 16, 16])\n",
      "\u001b[32mDimensions of out(weights @ v):\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[22;31mHead Block\u001b[0m\n",
      "\u001b[32mDimensions of input:\u001b[0m torch.Size([1, 16, 720])\n",
      "\u001b[32mDimensions of key:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of query:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of value:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of weights after q @ k.T:\u001b[0m torch.Size([1, 16, 16])\n",
      "\u001b[32mDimensions of out(weights @ v):\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[22;31mHead Block\u001b[0m\n",
      "\u001b[32mDimensions of input:\u001b[0m torch.Size([1, 16, 720])\n",
      "\u001b[32mDimensions of key:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of query:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of value:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of weights after q @ k.T:\u001b[0m torch.Size([1, 16, 16])\n",
      "\u001b[32mDimensions of out(weights @ v):\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[22;31mHead Block\u001b[0m\n",
      "\u001b[32mDimensions of input:\u001b[0m torch.Size([1, 16, 720])\n",
      "\u001b[32mDimensions of key:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of query:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of value:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of weights after q @ k.T:\u001b[0m torch.Size([1, 16, 16])\n",
      "\u001b[32mDimensions of out(weights @ v):\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[22;31mHead Block\u001b[0m\n",
      "\u001b[32mDimensions of input:\u001b[0m torch.Size([1, 16, 720])\n",
      "\u001b[32mDimensions of key:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of query:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of value:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of weights after q @ k.T:\u001b[0m torch.Size([1, 16, 16])\n",
      "\u001b[32mDimensions of out(weights @ v):\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[22;31mHead Block\u001b[0m\n",
      "\u001b[32mDimensions of input:\u001b[0m torch.Size([1, 16, 720])\n",
      "\u001b[32mDimensions of key:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of query:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of value:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of weights after q @ k.T:\u001b[0m torch.Size([1, 16, 16])\n",
      "\u001b[32mDimensions of out(weights @ v):\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[22;31mHead Block\u001b[0m\n",
      "\u001b[32mDimensions of input:\u001b[0m torch.Size([1, 16, 720])\n",
      "\u001b[32mDimensions of key:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of query:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of value:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of weights after q @ k.T:\u001b[0m torch.Size([1, 16, 16])\n",
      "\u001b[32mDimensions of out(weights @ v):\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[22;31mHead Block\u001b[0m\n",
      "\u001b[32mDimensions of input:\u001b[0m torch.Size([1, 16, 720])\n",
      "\u001b[32mDimensions of key:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of query:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of value:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of weights after q @ k.T:\u001b[0m torch.Size([1, 16, 16])\n",
      "\u001b[32mDimensions of out(weights @ v):\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[22;31mHead Block\u001b[0m\n",
      "\u001b[32mDimensions of input:\u001b[0m torch.Size([1, 16, 720])\n",
      "\u001b[32mDimensions of key:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of query:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of value:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of weights after q @ k.T:\u001b[0m torch.Size([1, 16, 16])\n",
      "\u001b[32mDimensions of out(weights @ v):\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[22;31mHead Block\u001b[0m\n",
      "\u001b[32mDimensions of input:\u001b[0m torch.Size([1, 16, 720])\n",
      "\u001b[32mDimensions of key:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of query:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of value:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of weights after q @ k.T:\u001b[0m torch.Size([1, 16, 16])\n",
      "\u001b[32mDimensions of out(weights @ v):\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[22;31mHead Block\u001b[0m\n",
      "\u001b[32mDimensions of input:\u001b[0m torch.Size([1, 16, 720])\n",
      "\u001b[32mDimensions of key:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of query:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of value:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of weights after q @ k.T:\u001b[0m torch.Size([1, 16, 16])\n",
      "\u001b[32mDimensions of out(weights @ v):\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[22;31mHead Block\u001b[0m\n",
      "\u001b[32mDimensions of input:\u001b[0m torch.Size([1, 16, 720])\n",
      "\u001b[32mDimensions of key:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of query:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of value:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of weights after q @ k.T:\u001b[0m torch.Size([1, 16, 16])\n",
      "\u001b[32mDimensions of out(weights @ v):\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[22;31mHead Block\u001b[0m\n",
      "\u001b[32mDimensions of input:\u001b[0m torch.Size([1, 16, 720])\n",
      "\u001b[32mDimensions of key:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of query:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of value:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of weights after q @ k.T:\u001b[0m torch.Size([1, 16, 16])\n",
      "\u001b[32mDimensions of out(weights @ v):\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[22;31mHead Block\u001b[0m\n",
      "\u001b[32mDimensions of input:\u001b[0m torch.Size([1, 16, 720])\n",
      "\u001b[32mDimensions of key:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of query:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of value:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of weights after q @ k.T:\u001b[0m torch.Size([1, 16, 16])\n",
      "\u001b[32mDimensions of out(weights @ v):\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[22;31mHead Block\u001b[0m\n",
      "\u001b[32mDimensions of input:\u001b[0m torch.Size([1, 16, 720])\n",
      "\u001b[32mDimensions of key:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of query:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of value:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of weights after q @ k.T:\u001b[0m torch.Size([1, 16, 16])\n",
      "\u001b[32mDimensions of out(weights @ v):\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[22;31mHead Block\u001b[0m\n",
      "\u001b[32mDimensions of input:\u001b[0m torch.Size([1, 16, 720])\n",
      "\u001b[32mDimensions of key:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of query:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of value:\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[32mDimensions of weights after q @ k.T:\u001b[0m torch.Size([1, 16, 16])\n",
      "\u001b[32mDimensions of out(weights @ v):\u001b[0m torch.Size([1, 16, 45])\n",
      "\u001b[22;31mMultiHead Block\u001b[0m\n",
      "Dimensions of output: torch.Size([1, 16, 720])\n"
     ]
    }
   ],
   "source": [
    "g = GPTModel(vocab_size = VOCAB_SIZE, \n",
    "             embd = EMBEDDING_DIM, \n",
    "             block_size = BLOCK_SIZE, \n",
    "             n_head = N_HEADS,\n",
    "             dim_journey=True)\n",
    "g.to(device)\n",
    "temp = g(inputs[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "decc3e87-6b7a-4d8e-8059-fc139669d097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16]) torch.Size([1, 16, 25454])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0:1].shape, temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813a13d3-1b95-4428-9ee5-98c0a6d5a30d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
